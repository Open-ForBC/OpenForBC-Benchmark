<?xml version="1.0"?>
<!--Phoronix Test Suite v9.8.0m1-->
<PhoronixTestSuite>
  <TestInformation>
    <Title>oneDNN</Title>
    <AppVersion>1.5</AppVersion>
    <Description>This is a test of the Intel oneDNN as an Intel-optimized library for Deep Neural Networks and making use of its built-in benchdnn functionality. The result is the total perf time reported. Intel oneDNN was formerly known as DNNL (Deep Neural Network Library) and MKL-DNN before being rebranded as part of the oneAPI initiative.</Description>
    <ResultScale>ms</ResultScale>
    <Proportion>LIB</Proportion>
    <TimesToRun>3</TimesToRun>
  </TestInformation>
  <TestProfile>
    <Version>1.5.0</Version>
    <SupportedPlatforms>Linux</SupportedPlatforms>
    <SoftwareType>Utility</SoftwareType>
    <TestType>Processor</TestType>
    <License>Free</License>
    <Status>Verified</Status>
    <ExternalDependencies>build-utilities, cmake</ExternalDependencies>
    <EnvironmentSize>287</EnvironmentSize>
    <ProjectURL>https://github.com/oneapi-src/oneDNN</ProjectURL>
    <InternalTags>SMP, OpenMP</InternalTags>
    <Maintainer>Michael Larabel</Maintainer>
  </TestProfile>
  <TestSettings>
    <Option>
      <DisplayName>Harness</DisplayName>
      <Identifier>harness</Identifier>
      <Menu>
        <Entry>
          <Name>Convolution Batch Shapes Auto</Name>
          <Value>--conv --batch=inputs/conv/shapes_auto</Value>
        </Entry>
        <Entry>
          <Name>Deconvolution Batch deconv_1d</Name>
          <Value>--deconv --batch=inputs/deconv/deconv_1d</Value>
        </Entry>
        <Entry>
          <Name>Deconvolution Batch deconv_3d</Name>
          <Value>--deconv --batch=inputs/deconv/deconv_3d</Value>
        </Entry>
        <Entry>
          <Name>IP Batch 1D</Name>
          <Value>--ip --batch=inputs/ip/ip_1d</Value>
        </Entry>
        <Entry>
          <Name>IP Batch All</Name>
          <Value>--ip --batch=inputs/ip/ip_all</Value>
        </Entry>
        <Entry>
          <Name>Matrix Multiply Batch Shapes Transformer</Name>
          <Value>--matmul --batch=inputs/matmul/shapes_transformer</Value>
        </Entry>
        <Entry>
          <Name>Recurrent Neural Network Training</Name>
          <Value>--rnn --batch=inputs/rnn/rnn_training</Value>
        </Entry>
        <Entry>
          <Name>Recurrent Neural Network Inference</Name>
          <Value>--rnn --batch=inputs/rnn/rnn_inference</Value>
        </Entry>
      </Menu>
    </Option>
    <Option>
      <DisplayName>Data Type</DisplayName>
      <Identifier>data-type</Identifier>
      <ArgumentPrefix>--cfg=</ArgumentPrefix>
      <Menu>
        <Entry>
          <Name>f32</Name>
          <Value>f32</Value>
        </Entry>
        <Entry>
          <Name>u8s8f32</Name>
          <Value>u8s8f32</Value>
          <Message>Optimized For AVX-512</Message>
        </Entry>
        <Entry>
          <Name>bf16bf16bf16</Name>
          <Value>bf16bf16bf16</Value>
          <Message>Optimized For AVX-512 + VNNI</Message>
        </Entry>
      </Menu>
    </Option>
    <Option>
      <DisplayName>Engine</DisplayName>
      <Identifier>engine</Identifier>
      <Menu>
        <Entry>
          <Name>CPU</Name>
          <Value>--engine=cpu</Value>
        </Entry>
      </Menu>
    </Option>
  </TestSettings>
</PhoronixTestSuite>
