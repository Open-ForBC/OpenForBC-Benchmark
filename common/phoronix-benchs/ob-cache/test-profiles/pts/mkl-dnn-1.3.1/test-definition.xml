<?xml version="1.0"?>
<!--Phoronix Test Suite v9.8.0m1-->
<PhoronixTestSuite>
  <TestInformation>
    <Title>oneDNN MKL-DNN</Title>
    <AppVersion>1.3</AppVersion>
    <Description>This is a test of the Intel oneDNN (formerly DNNL / Deep Neural Network Library / MKL-DNN) as an Intel-optimized library for Deep Neural Networks and making use of its built-in benchdnn functionality. The result is the total perf time reported.</Description>
    <ResultScale>ms</ResultScale>
    <Proportion>LIB</Proportion>
    <TimesToRun>3</TimesToRun>
    <PreInstallMessage>The pts/mkl-dnn test profile has been succeeded by the pts/onednn test profile to match the naming convetion used by Intel following their renaming to oneDNN. Run pts/onednn test profile to use the latest version of Intel oneDNN benchmark.</PreInstallMessage>
  </TestInformation>
  <TestProfile>
    <Version>1.3.1</Version>
    <SupportedPlatforms>Linux</SupportedPlatforms>
    <SoftwareType>Utility</SoftwareType>
    <TestType>Processor</TestType>
    <License>Free</License>
    <Status>Verified</Status>
    <ExternalDependencies>build-utilities, cmake</ExternalDependencies>
    <EnvironmentSize>355</EnvironmentSize>
    <ProjectURL>https://github.com/oneapi-src/oneDNN</ProjectURL>
    <InternalTags>SMP, OpenMP</InternalTags>
    <Maintainer>Michael Larabel</Maintainer>
  </TestProfile>
  <TestSettings>
    <Option>
      <DisplayName>Harness</DisplayName>
      <Identifier>harness</Identifier>
      <Menu>
        <Entry>
          <Name>Deconvolution Batch deconv_1d</Name>
          <Value>--deconv --batch=inputs/deconv/deconv_1d</Value>
        </Entry>
        <Entry>
          <Name>Deconvolution Batch deconv_3d</Name>
          <Value>--deconv --batch=inputs/deconv/deconv_3d</Value>
        </Entry>
        <Entry>
          <Name>IP Batch 1D</Name>
          <Value>--ip --batch=inputs/ip/ip_1d</Value>
        </Entry>
        <Entry>
          <Name>IP Batch All</Name>
          <Value>--ip --batch=inputs/ip/ip_all</Value>
        </Entry>
        <Entry>
          <Name>Recurrent Neural Network Training</Name>
          <Value>--rnn --batch=inputs/rnn/rnn_training</Value>
        </Entry>
        <Entry>
          <Name>Recurrent Neural Network Inference</Name>
          <Value>--rnn --batch=inputs/rnn/rnn_inference</Value>
        </Entry>
      </Menu>
    </Option>
    <Option>
      <DisplayName>Data Type</DisplayName>
      <Identifier>data-type</Identifier>
      <ArgumentPrefix>--cfg=</ArgumentPrefix>
      <Menu>
        <Entry>
          <Name>f32</Name>
          <Value>f32</Value>
        </Entry>
        <Entry>
          <Name>u8s8f32</Name>
          <Value>u8s8f32</Value>
          <Message>Optimized For AVX-512</Message>
        </Entry>
        <Entry>
          <Name>bf16bf16bf16</Name>
          <Value>bf16bf16bf16</Value>
          <Message>Optimized For AVX-512 + VNNI</Message>
        </Entry>
      </Menu>
    </Option>
  </TestSettings>
</PhoronixTestSuite>
